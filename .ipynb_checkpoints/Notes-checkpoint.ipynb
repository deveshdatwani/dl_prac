{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebf539a",
   "metadata": {},
   "source": [
    "DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5d004",
   "metadata": {},
   "source": [
    "-- empty -- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb6422",
   "metadata": {},
   "source": [
    "OVERFITTING\n",
    "\n",
    "    The difference in accuracy between training and validation accuracy is noticeableâ€”a sign of overfitting.\n",
    "\n",
    "    There are multiple ways to fight overfitting in the training process. Two of them are data augmentation and adding a dropout.\n",
    "\n",
    "    Data Augmentation: Overfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "    Dropout Regularization: When you apply dropout to a layer, it randomly drops out (by setting the activation to zero) a number of output units from the layer during the training process. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer. Dropout forces the model to learn more robust features of the data as well as reducing the model to develop interdependencies between nodes. The epochs needed are doubled but the training time per epoch is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cf339",
   "metadata": {},
   "source": [
    "TUNING \n",
    "\n",
    "1. overfitting\n",
    "    Dropout\n",
    "    Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829e8bc",
   "metadata": {},
   "source": [
    "TECHNIQUES\n",
    "\n",
    "Normalization\n",
    "Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc63d6d",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS\n",
    "\n",
    "Weight initialization\n",
    "Optimizer\n",
    "Metric\n",
    "Loss value\n",
    "Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bd478",
   "metadata": {},
   "source": [
    "NONLINEARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfe1b1",
   "metadata": {},
   "source": [
    "IMBALANCED DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afcf1b",
   "metadata": {},
   "source": [
    "LOSS FUNCTIONS\n",
    "\n",
    "1. Regression Loss Functions\n",
    "    Mean Squared Error Loss\n",
    "    Mean Squared Logarithmic Error Loss\n",
    "    Mean Absolute Error Loss\n",
    "\n",
    "2. Binary Classification Loss Functions\n",
    "    Binary Cross-Entropy\n",
    "    Hinge Loss\n",
    "    Squared Hinge Loss\n",
    "\n",
    "3. Multi-Class Classification Loss Functions\n",
    "    Multi-Class Cross-Entropy Loss\n",
    "    Sparse Multiclass Cross-Entropy Loss\n",
    "    Kullback Leibler Divergence Loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
