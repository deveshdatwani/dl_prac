{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebf539a",
   "metadata": {},
   "source": [
    "DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5d004",
   "metadata": {},
   "source": [
    "-- empty -- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cf339",
   "metadata": {},
   "source": [
    "OVERFITTING\n",
    "\n",
    "The difference in accuracy between training and validation accuracy is noticeableâ€”a sign of overfitting.\n",
    "\n",
    "There are multiple ways to fight overfitting in the training process. Two of them are data augmentation and adding a dropout.\n",
    "\n",
    "Regularization: \n",
    "\n",
    "Large weights make the network unstable. Although the weight will be specialized to the training dataset, minor variation or statistical noise on the expected inputs will result in large differences in the output. \n",
    "\n",
    "Large weights tend to cause sharp transitions in the node functions and thus large changes in output for small changes in the inputs.\n",
    "\n",
    "A model with large weights is more complex than a model with smaller weights. It is a sign of a network that may be overly specialized to training data. \n",
    "\n",
    "Methods:\n",
    "\n",
    "Dropout layer\n",
    "Data augmentation\n",
    "\n",
    "Data Augmentation: Overfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "Dropout Regularization: When you apply dropout to a layer, it randomly drops out (by setting the activation to zero) a number of output units from the layer during the training process. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer. Dropout forces the model to learn more robust features of the data as well as reducing the model to develop interdependencies between nodes. The epochs needed are doubled but the training time per epoch is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829e8bc",
   "metadata": {},
   "source": [
    "TECHNIQUES\n",
    "\n",
    "Normalization\n",
    "Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc63d6d",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS\n",
    "\n",
    "Weight initialization\n",
    "Optimizer\n",
    "Metric\n",
    "Loss function\n",
    "Activation function\n",
    "Number of filters\n",
    "Number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bd478",
   "metadata": {},
   "source": [
    "NONLINEARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfe1b1",
   "metadata": {},
   "source": [
    "IMBALANCED DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afcf1b",
   "metadata": {},
   "source": [
    "LOSS FUNCTIONS\n",
    "\n",
    "1. Regression Loss Functions\n",
    "    Mean Squared Error Loss\n",
    "    Mean Squared Logarithmic Error Loss\n",
    "    Mean Absolute Error Loss\n",
    "\n",
    "2. Binary Classification Loss Functions\n",
    "    Binary Cross-Entropy\n",
    "    Hinge Loss\n",
    "    Squared Hinge Loss\n",
    "\n",
    "3. Multi-Class Classification Loss Functions\n",
    "    Multi-Class Cross-Entropy Loss\n",
    "    Sparse Multiclass Cross-Entropy Loss\n",
    "    Kullback Leibler Divergence Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571351c9",
   "metadata": {},
   "source": [
    "BACKPROPOGATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbe390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
